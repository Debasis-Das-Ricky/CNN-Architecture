{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture"
      ],
      "metadata": {
        "id": "ayYLtttfpWPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?\n",
        "\n",
        " Ans. Filters in a Convolutional Neural Network (CNN) are small matrices of learnable weights that slide over the input data (such as an image) to detect specific local patterns, like edges, textures, or shapes. Each filter is designed, through the learning process, to extract certain features by performing a convolution operation—multiplying the filter with patches of the input and summing up the result. This process produces a new matrix called a feature map.\n",
        "\n",
        " Feature maps are the outputs generated by applying filters to the input data. Each feature map highlights the presence and location of a particular feature that the corresponding filter is specialized to detect. In early layers of a CNN, feature maps might capture simple features like edges or colors, while deeper layers capture more complex and abstract patterns by combining information from previous layers.\n",
        "\n",
        "\n",
        "2. Explain the concepts of padding and stride in CNNs(Convolutional Neural Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        " Ans. Padding and stride are fundamental concepts in Convolutional Neural Networks (CNNs) that control how filters move over input data and determine the dimensions of the output feature maps produced by convolutional layers.\n",
        "\n",
        " Padding refers to adding extra pixels (usually zeros) around the border of the input data before the convolution operation. The main reason for using padding is to preserve the spatial dimensions of the input, especially when using bigger filters. Without padding, the output feature map becomes smaller than the input, which could lead to a rapid reduction in feature map size after several layers and possible loss of border information. Common types of padding include 'valid' (no padding, leading to smaller outputs) and 'same' (padding such that the output size remains the same as the input size).\n",
        "\n",
        " Stride is the step size with which the filter moves across the input data. A stride of 1 means the filter shifts one pixel at a time, leading to large and detailed output feature maps. Increasing the stride (e.g., stride = 2) causes the filter to move by more than one pixel, reducing the size of the output feature map and effectively downsampling the input.\n",
        "\n",
        " Both padding and stride directly affect the output dimensions. The general formula for calculating the size of the output feature map  is:\n",
        "\n",
        "                  O = ((n + 2p - f)/s) + 1\n",
        "\n",
        " where O is the output size, n is the input size, p is the padding applied, f is the filter (kernel) size, and s is the stride. Adding padding increases the output size, while increasing stride reduces it.\n",
        "\n",
        "\n",
        "\n",
        "3. Define receptive field in the context of CNNs. Why is it important for deep architectures?\n",
        "\n",
        " Ans. In Convolutional Neural Networks (CNNs), the receptive field refers to the specific region of the input data (such as an image) that a particular neuron or feature in a given layer \"sees\" or is influenced by. For the first convolutional layer, a neuron's receptive field might be a small patch, like 3×3 pixels. In deeper layers, each neuron's receptive field is composed of multiple patches from earlier layers, causing it to span a much larger area of the original input. This expansion allows the network to hierarchically learn from low-level local features to more complex and abstract representations of objects.\n",
        "\n",
        " The size of the receptive field is crucial for deep CNN architectures because it determines how much context the network uses to make predictions. If the receptive field is too small, neurons may only detect local patterns and miss larger objects or global context, which limits performance on tasks like object detection or scene understanding. Conversely, large receptive fields allow deep layers to capture relationships across the entire input, enabling the model to recognize complex shapes, objects, or even scenes by aggregating information from different regions.\n",
        "\n",
        " When designing deep architectures, a properly sized receptive field ensures that the network can adequately model the scale of objects present in the data. Techniques like increasing the number of layers, using larger kernels, or employing dilated convolutions are common ways to expand the receptive field without excessive computation or parameter growth. Thus, the receptive field helps balance local feature detection and global context understanding, which is essential for effective image analysis and high-level vision tasks.\n",
        "\n",
        "\n",
        "4. Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        " Ans. Filter size in a CNN determines the dimensions of the region each filter covers when scanning the input volume (for example, a 3x3 filter covers a 3-by-3 area). The number of parameters in a convolutional layer is directly influenced by the size of the filter: for each filter, the total parameters are calculated as\n",
        " k1 x k2 x C + 1, where k1 and k2 are filter width and height, C is the number of input channels, and the '+1' accounts for the bias term. Thus, larger filters have more weights, increasing the number of parameters in the layer.\n",
        "\n",
        " Stride refers to how far the filter shifts across the input with each move. Importantly, stride does not affect the number of parameters in a convolutional layer because the filter weights remain the same regardless of how often they are applied. Changing stride only alters the output feature map's dimensions: a large stride produces a smaller feature map and fewer activations, while a stride of one creates a larger map, but the weights and biases (the parameters) for the filter remain unchanged.\n",
        "\n",
        "\n",
        "\n",
        "5. Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        " Ans. LeNet, AlexNet, and VGG: Comparing CNN Architectures:\n",
        "\n",
        " Depth: LeNet is one of the earliest and shallowest CNN architectures, typically composed of seven layers including convolution, pooling, and fully connected layers. AlexNet increased depth and complexity significantly, using eight layers (five convolutional and three fully connected), which helped it achieve breakthrough accuracy on large-scale datasets like ImageNet. VGG pushed the concept of deep learning further, featuring 16 or 19 layers (VGG16/VGG19), mostly convolutional layers stacked deeper than previous models.\n",
        "\n",
        " Filter Sizes: LeNet predominantly uses larger filters (e.g., 5 x 5) at its convolutional layers. AlexNet employs various filter sizes, including larger initial filters (often 11 x 11 and 5 x 5) followed by smaller ones. VGG, however, popularized the use of small 3 x 3 filters throughout its entire network, often stacking them to increase depth and capture more complex features. The choice of smaller, uniform filters in VGG has been shown to enhance feature extraction and generalization while keeping individual layers computationally manageable.\n",
        "\n",
        " Performance: LeNet works well for simple image classification tasks (like handwritten digit recognition) but is less effective on large, complex datasets due to its shallow structure. AlexNet, with its increased depth, use of ReLU activations, and techniques like dropout and max pooling, delivered dramatic improvements on large-scale images and won the ILSVRC 2012 competition. VGG's much deeper and uniform structure gave even higher accuracy on benchmarks such as ImageNet, with VGG-16 attaining about 92.7% top-5 accuracy. However, VGG models have a high computational demand, requiring more memory and training time due to the large number of parameters (over 130 million for VGG16).\n"
      ],
      "metadata": {
        "id": "_GEfMQcLpYxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)  # Shape: (num_samples, 28, 28, 1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Build the CNN model\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=5,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAKLukW041St",
        "outputId": "f6314953-da59-4d7c-d3ea-91ea43f1fbb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "422/422 - 43s - 102ms/step - accuracy: 0.9303 - loss: 0.2379 - val_accuracy: 0.9807 - val_loss: 0.0630\n",
            "Epoch 2/5\n",
            "422/422 - 43s - 101ms/step - accuracy: 0.9816 - loss: 0.0616 - val_accuracy: 0.9858 - val_loss: 0.0534\n",
            "Epoch 3/5\n",
            "422/422 - 41s - 96ms/step - accuracy: 0.9863 - loss: 0.0444 - val_accuracy: 0.9883 - val_loss: 0.0445\n",
            "Epoch 4/5\n",
            "422/422 - 41s - 97ms/step - accuracy: 0.9895 - loss: 0.0333 - val_accuracy: 0.9882 - val_loss: 0.0402\n",
            "Epoch 5/5\n",
            "422/422 - 41s - 98ms/step - accuracy: 0.9918 - loss: 0.0266 - val_accuracy: 0.9888 - val_loss: 0.0378\n",
            "Test accuracy: 0.9888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess the images: scale pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define a simple CNN model for CIFAR-10\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Optionally, compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "TcZXwH0a5Bm5",
        "outputId": "49fe66a3-c388-42be-f120-fcc3a34c50f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,048,704\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,704</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,143,242\u001b[0m (4.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,143,242</span> (4.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,143,242\u001b[0m (4.36 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,143,242</span> (4.36 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8)  Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data transforms and loaders\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # converts data to PyTorch tensor and normalizes to [0,1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  # output: 32x28x28\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)              # output: 32x14x14\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # output: 64x14x14\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)              # output: 64x7x7\n",
        "        self.fc1 = nn.Linear(64*7*7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation (accuracy on test set)\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e84vFKh5f9o",
        "outputId": "0e659c80-f213-4180-c502-3f7df7bcf712"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.25MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 159kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.69MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.1830\n",
            "Epoch 2/5, Loss: 0.0476\n",
            "Epoch 3/5, Loss: 0.0348\n",
            "Epoch 4/5, Loss: 0.0257\n",
            "Epoch 5/5, Loss: 0.0197\n",
            "Test Accuracy: 99.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Create a dummy dataset directory for demonstration\n",
        "dataset_dir = 'my_dummy_dataset'\n",
        "if not os.path.exists(dataset_dir):\n",
        "    os.makedirs(dataset_dir)\n",
        "    # Create dummy class subdirectories\n",
        "    class_names = ['class1', 'class2', 'class3']\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(dataset_dir, class_name)\n",
        "        os.makedirs(class_path, exist_ok=True)\n",
        "        # Create some dummy images in each class\n",
        "        for i in range(5):\n",
        "            dummy_image = np.random.randint(0, 255, (150, 150, 3), dtype=np.uint8)\n",
        "            img = Image.fromarray(dummy_image)\n",
        "            img.save(os.path.join(class_path, f'image_{i}.png'))\n",
        "    print(f\"Created dummy dataset at '{dataset_dir}'\")\n",
        "else:\n",
        "    print(f\"Dummy dataset directory '{dataset_dir}' already exists.\")\n",
        "\n",
        "# ImageDataGenerator for preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Reserve 20% data for validation\n",
        ")\n",
        "\n",
        "# Creating train and validation generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Define a simple CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# Note: With a tiny dummy dataset, training might not be meaningful or may raise warnings.\n",
        "# This is just to demonstrate the code runs without FileNotFoundError.\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=1,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrALiEy76PJ4",
        "outputId": "9f03e4a4-8e24-43f5-ebb9-c199e69d0120"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dummy dataset at 'my_dummy_dataset'\n",
            "Found 12 images belonging to 3 classes.\n",
            "Found 3 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 4s - 4s/step - accuracy: 0.3333 - loss: 1.1013 - val_accuracy: 0.3333 - val_loss: 2.8586\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cf877bc6120>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach-from data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        " Ans.\n",
        "\n",
        " Data Preparation:\n",
        "\n",
        "  Dataset Collection: Gather a labeled dataset of chest X-ray images, sorted into \"Normal\" and \"Pneumonia\" folders. A popular public dataset is the \"Chest X-ray Images (Pneumonia)\" dataset from Kaggle.\n",
        "\n",
        "\n",
        " Data Directory Structure:\n",
        "\n",
        "      dataset/\n",
        "        train/\n",
        "          Normal/\n",
        "          Pneumonia/\n",
        "        val/\n",
        "          Normal/\n",
        "          Pneumonia/\n",
        "        test/\n",
        "          Normal/\n",
        "          Pneumonia/\n",
        "\n",
        "\n",
        " Preprocessing and Augmentation: Use tools like Keras'ImageDataGenerator to:\n",
        "    \n",
        "    a. Rescale pixel values to.\n",
        "    b. Apply real-time augmentation (rotations, zoom, horizontal flips) to reduce overfitting.\n",
        "    c. Resize images to a suitable input size (e.g., 224×224).\n",
        "\n",
        " Model Building and Training:\n",
        "\n",
        "  Model Architecture: Design a CNN or use transfer learning with pretrained models (e.g., MobileNet, ResNet) for better performance on small medical datasets.\n",
        "\n",
        "  Example (Transfer Learning):\n",
        "\n",
        "  \n",
        "\n",
        "```\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Freeze base model layers at first\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "```\n",
        "\n",
        " Training:\n",
        "\n",
        "  a. Use model.fit() with your training and validation data generators.\n",
        "\n",
        "  b. Monitor validation accuracy and use callbacks like ModelCheckpoint and EarlyStopping.\n",
        "\n",
        "\n",
        " Model Evaluation and Saving\n",
        "   \n",
        "   a. Evaluate the model using the separate test set and record the metrics (accuracy, precision, recall, AUC).\n",
        "\n",
        "   b. Save the trained model as an .h5 or .keras file:\n",
        "          \n",
        "          model.save(\"chest_xray_cnn_model.h5\")\n",
        "\n",
        " Deploying as a Web App (Streamlit)\n",
        "\n",
        "   a. Setup Streamlit Project: Install Streamlit (pip install streamlit) and create an app.py file.\n",
        "\n",
        "   b. App Interface:\n",
        "\n",
        "       i. Upload X-ray image.\n",
        "       ii. Preprocess the image as during model training (resize, scale).\n",
        "       iii. Load the saved model using keras.models.load_model.\n",
        "       iv. Perform prediction and display the result (\"Normal\" or \"Pneumonia\") with probability.\n",
        "\n",
        "   c. Streamlit Example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('chest_xray_cnn_model.h5')\n",
        "\n",
        "st.title(\"Chest X-ray Classifier\")\n",
        "uploaded_file = st.file_uploader(\"Upload a Chest X-ray image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file).convert('RGB').resize((224, 224))\n",
        "    st.image(image, caption='Uploaded Image', use_column_width=True)\n",
        "    img_array = np.array(image) / 255.0\n",
        "    img_expanded = np.expand_dims(img_array, axis=0)\n",
        "    prediction = model.predict(img_expanded)[0][0]\n",
        "    label = \"Pneumonia\" if prediction > 0.5 else \"Normal\"\n",
        "    st.write(f\"Prediction: **{label}** (Probability: {prediction:.2f})\")\n",
        "\n",
        "```\n",
        "\n",
        "  Deployment\n",
        "\n",
        "     a. Deploy the Streamlit app to a cloud service (e.g., Streamlit Community Cloud, Heroku, AWS EC2, or Google Cloud Run).\n",
        "     b. Ensure your requirements (requirements.txt) and model file are included.\n",
        "     c. Optionally, secure the app and monitor for performance/reliability.\n"
      ],
      "metadata": {
        "id": "ZiGVmZriA75D"
      }
    }
  ]
}